# LLM-analysis

This repository contains the experimental framework and evaluation results for the study: "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern Large Language Models".

## Overview
The goal of the research is to investigate how various prompting strategies and code context levels influence the quality of unit tests generated by Large Language Models (LLMs).
I evaluate multiple LLMs, prompt styles, and code context variants using both quantitative and qualitative metrics.

## Research Focus

- Comparison of **Simple Prompting** and **Chain-of-Thought Prompting** strategies.
- Evaluation across different code context levels:
  - **Method signatures only**: Only method signatures are provided to the LLM.
  - **Method signatures + docstrings**: Method signatures and docstrings are provided to the LLM.
  - **Full context**: The entire code file is provided to the LLM.
- Model testes:
  - **GPT o3**
  - **GPT o4-mini-high**
  - **GPT 4.5**
  - **Gemini 2.5 Pro**
  - **Claude 3.7 Sonet**
  - **Deepseek**

## Repository Structure
```
LLM-analysis/
│
├── mutants/                            # mutmut.py config and class used for mutation testing
├── prompts_results/                    # Results of LLM prompts and analyses (including coverage.py and mutmut.py analyses)
│   ├── simple_prompting/               # Simple prompting strategy results
│   │   ├── interface/                  # Interface-level tests
│   │   ├── interface_docstring/        # Interface-level tests with docstrings
│   │   ├── full_context/               # Full context tests
│   └── chain_of_thought_prompting/     # Chain-of-Thought prompting strategy results
│       ├── interface/                  # Interface-level tests
│       ├── interface_docstring/        # Interface-level tests with docstrings
│       ├── full_context/               # Full context tests
├── order_calculator.py                 # Python class used in all test generations
├── scenariusze_testowe.md              # Markdown file with all test scenarios written manually at the beginning of the research
└── README.md
```

## Evaluation Metrics

### Quantitative
- **Code Coverage**: Measured using `coverage.py` to determine the percentage of code covered by generated tests.
- **Syntactic correctness**: Ensured that generated tests are syntactically correct and can be executed without errors.
- **Mutation score**: Measured using `mutmut.py` to evaluate the effectiveness of tests in detecting code mutations.
- **Test uniqueness**: Ensured that generated tests are unique and do not duplicate existing tests.
- **Generation latency**: Measured the time taken to generate tests for each code context and prompting strategy.

### Qualitative
- Input variety and coverage of edge cases.
- Clarity, atomicity, and naming of the test cases.
- Adherence to prompt instructions and expected behaviors.