#!/usr/bin/env python3
"""
Skrypt uruchamiajƒÖcy eksperymenty LLM
"""
import argparse
import logging
import json
from pathlib import Path
from experiment_runner import ExperimentRunner


def setup_logging():
    """Konfiguracja logowania"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('automation.log'),
            logging.StreamHandler()
        ]
    )


def start_chrome_for_debug():
    """Start Chrome with remote debugging (obs≈Çuguje WSL z poprawkami katalog√≥w)"""
    import subprocess
    import time
    import os

    # Sprawd≈∫ czy jeste≈õmy w WSL
    def is_wsl():
        try:
            with open('/proc/version', 'r') as f:
                return 'microsoft' in f.read().lower() or 'wsl' in f.read().lower()
        except:
            return False

    def create_chrome_data_dir():
        """Tworzy katalog danych Chrome z odpowiednimi uprawnieniami"""
        if is_wsl():
            # W WSL u≈ºywamy Windows temp directory jako g≈Ç√≥wnƒÖ opcjƒô
            # Chrome w Windows nie mo≈ºe zapisywaƒá do katalogu Linux home
            windows_temp = "/mnt/c/temp/chrome_debug"
            try:
                # Utw√≥rz katalog przez Windows cmd
                subprocess.run([
                    "cmd.exe", "/c", "mkdir", "C:\\temp\\chrome_debug"
                ], capture_output=True, check=False)  # ignore errors if exists
                
                # Sprawd≈∫ czy katalog istnieje
                if os.path.exists(windows_temp):
                    print(f"‚úì Using Windows temp directory: {windows_temp}")
                    return windows_temp
                else:
                    print(f"‚ö†Ô∏è  Windows temp directory not accessible: {windows_temp}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Windows temp creation failed: {e}")

            # Fallback 1: U≈ºyj Windows AppData
            try:
                windows_appdata = "/mnt/c/Users/" + os.environ.get('USER', 'ptomalak') + "/AppData/Local/Temp/chrome_debug"
                subprocess.run([
                    "cmd.exe", "/c", f"mkdir C:\\Users\\{os.environ.get('USER', 'ptomalak')}\\AppData\\Local\\Temp\\chrome_debug"
                ], capture_output=True, check=False)
                
                if os.path.exists(windows_appdata):
                    print(f"‚úì Using Windows AppData directory: {windows_appdata}")
                    return windows_appdata
            except Exception as e2:
                print(f"‚ö†Ô∏è  Windows AppData fallback failed: {e2}")

            # Fallback 2: u≈ºyj katalogu roboczego - Chrome mo≈ºe do niego pisaƒá
            work_dir = os.path.join(os.getcwd(), "chrome_debug_temp")
            os.makedirs(work_dir, exist_ok=True)
            print(f"‚úì Using working directory: {work_dir}")
            print("üìù Note: This directory will be accessible to Windows Chrome")
            return work_dir
        else:
            # Windows/Linux native
            temp_dir = "C:\\temp\\chrome_debug"
            try:
                os.makedirs(temp_dir, exist_ok=True)
                return temp_dir
            except:
                # Fallback do katalogu tymczasowego
                import tempfile
                temp_dir = os.path.join(tempfile.gettempdir(), "chrome_debug")
                os.makedirs(temp_dir, exist_ok=True)
                return temp_dir

    if is_wsl():
        print("üêß WSL detected - starting Windows Chrome from WSL")
        # W WSL u≈ºywaj Chrome z Windows
        chrome_paths = [
            '/mnt/c/Program Files/Google/Chrome/Application/chrome.exe',
            '/mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe'
        ]

        chrome_path = None
        for path in chrome_paths:
            if os.path.exists(path):
                chrome_path = path
                break

        if not chrome_path:
            print("‚ùå Chrome not found in Windows from WSL")
            return False

        # Utw√≥rz katalog danych
        data_dir = create_chrome_data_dir()

        cmd = [
            chrome_path,
            "--remote-debugging-port=9222",
            "--remote-debugging-address=0.0.0.0",  # Allow connections from WSL
            f"--user-data-dir={data_dir}",
            "--no-first-run",
            "--no-default-browser-check",
            "--disable-default-apps",
            "--disable-web-security",  # For automation
            "--disable-features=VizDisplayCompositor"  # WSL compatibility
        ]

    else:
        # Windows/Linux native
        chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
        data_dir = create_chrome_data_dir()

        cmd = [
            chrome_path,
            "--remote-debugging-port=9222",
            f"--user-data-dir={data_dir}",
            "--no-first-run",
            "--no-default-browser-check",
            "--disable-default-apps"
        ]

    try:
        # Uruchom Chrome w tle
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True
        )

        if is_wsl():
            print("‚úì Windows Chrome started from WSL with remote debugging on port 9222")
            print("üìù Automation will connect via WSL ‚Üí Windows bridge")
            print(f"üìÅ Data directory: {data_dir}")
        else:
            print("‚úì Chrome started with remote debugging on port 9222")
            print("üìù Empty browser opened - ready for automation")
            print(f"üìÅ Data directory: {data_dir}")

        time.sleep(3)  # Give Chrome time to start
        return True

    except Exception as e:
        print(f"‚úó Error starting Chrome: {e}")
        print("üí° Try running: sudo apt update && sudo apt install -y google-chrome-stable")
        return False


def get_model_url(model_name):
    """Get the URL for specific model with direct model selection"""
    model_urls = {
        'gpt-4.5': 'https://chatgpt.com/?model=gpt-4.5',
        'gpt-o3': 'https://chatgpt.com/?model=o3',
        'gpt-o4-mini-high': 'https://chatgpt.com/?model=o4-mini-high',
        'claude-3.7-sonnet': 'https://claude.ai/',
        'deepseek': 'https://chat.deepseek.com/',
        'gemini-2.5-pro': 'https://gemini.google.com/'
    }
    return model_urls.get(model_name, 'https://google.com')


def interactive_mode(runner):
    """Interactive mode"""
    print("\n" + "=" * 50)
    print("ü§ñ LLM TESTING AUTOMATION")
    print("=" * 50)

    # Sprawd≈∫ czy jeste≈õmy w WSL
    def is_wsl():
        try:
            with open('/proc/version', 'r') as f:
                return 'microsoft' in f.read().lower() or 'wsl' in f.read().lower()
        except:
            return False

    wsl_detected = is_wsl()
    if wsl_detected:
        print("\nüêß WSL DETECTED - Enhanced WSL Support Enabled")

    # Step 1: Browser setup
    print("\nüì± STEP 1: Browser setup")
    if wsl_detected:
        print("1. Start Windows Chrome from WSL (recommended)")
        print("2. I have Chrome debug already running")
    else:
        print("1. Start new Chrome with debug")
        print("2. I have Chrome debug already running")

    try:
        browser_choice = input("\nChoice (1-2): ").strip()

        if browser_choice == "1":
            if not start_chrome_for_debug():
                print("‚úó Failed to start Chrome")
                return
        elif browser_choice == "2":
            print("‚úì Using existing debug browser")
        else:
            print("‚úó Invalid choice")
            return

        # Step 2: Model selection
        print("\nüß† STEP 2: Model selection")
        models = [
            'gpt-4.5', 'gpt-o3', 'gpt-o4-mini-high',
            'claude-3.7-sonnet', 'deepseek', 'gemini-2.5-pro'
        ]

        for i, model in enumerate(models, 1):
            print(f"{i}. {model}")

        model_choice = int(input("\nSelect model (1-6): ")) - 1
        if model_choice < 0 or model_choice >= len(models):
            raise ValueError("Invalid model choice")

        # Step 3: Strategy
        print("\nüéØ STEP 3: Prompting strategy")
        strategies = ['simple_prompting', 'chain_of_thought_prompting']
        for i, strategy in enumerate(strategies, 1):
            print(f"{i}. {strategy.replace('_', ' ').title()}")

        strategy_choice = int(input("\nSelect strategy (1-2): ")) - 1
        if strategy_choice < 0 or strategy_choice >= len(strategies):
            raise ValueError("Invalid strategy")

        # Step 4: Context
        print("\nüìã STEP 4: Code context level")
        contexts = ['interface', 'interface_docstring', 'full_context']
        context_descriptions = [
            'Interface - method signatures only',
            'Interface + Docstring - signatures with documentation',
            'Full Context - complete source code'
        ]

        for i, (context, desc) in enumerate(zip(contexts, context_descriptions), 1):
            print(f"{i}. {desc}")

        context_choice = int(input("\nSelect context (1-3): ")) - 1
        if context_choice < 0 or context_choice >= len(contexts):
            raise ValueError("Invalid context")

        # Summary
        selected_model = models[model_choice]
        selected_strategy = strategies[strategy_choice]
        selected_context = contexts[context_choice]

        print("\n" + "=" * 50)
        print("üìä EXPERIMENT SUMMARY")
        print("=" * 50)
        print(f"üß† Model: {selected_model}")
        print(f"üéØ Strategy: {selected_strategy.replace('_', ' ').title()}")
        print(f"üìã Context: {selected_context}")
        print(f"üåê URL: {get_model_url(selected_model)}")
        print(f"üîç Model verification: Enhanced model checking enabled")

        if wsl_detected:
            print("üêß Platform: WSL (Windows Subsystem for Linux)")
            print("üîó Mode: WSL ‚Üí Windows Chrome bridge")
        else:
            print("üîó Mode: Attach to existing browser")

        confirm = input("\nüöÄ Run experiment? (y/N): ").strip().lower()

        if confirm != 'y':
            print("‚ùå Cancelled")
            return

        # Execution
        print("\nüöÄ STARTING EXPERIMENT...")
        print("-" * 50)

        # Always use attach mode now
        use_headless = False

        result = runner.run_single_experiment(
            selected_model, selected_strategy, selected_context,
            headless=use_headless
        )

        # Results
        print("\n" + "=" * 50)
        if result:
            print("‚úÖ EXPERIMENT COMPLETED SUCCESSFULLY!")
            print("=" * 50)

            if 'analysis' in result and result['analysis']:
                analysis = result['analysis']

                if 'compilation_success' in analysis:
                    status = "‚úÖ SUCCESS" if analysis['compilation_success'] else "‚ùå FAILED"
                    print(f"üîß Compilation: {status}")

                if 'coverage' in analysis and analysis['coverage']:
                    cov = analysis['coverage']
                    coverage_pct = cov.get('coverage_percent', 0)
                    print(f"üìä Code coverage: {coverage_pct}%")

                if 'scenarios' in analysis and analysis['scenarios']:
                    scen = analysis['scenarios']
                    test_count = scen.get('total_test_methods', 0)
                    print(f"üß™ Test count: {test_count}")

                print(f"\nüìÅ Results saved in: prompts_results/{selected_strategy}/{selected_context}/{selected_model}/")
        else:
            print("‚ùå EXPERIMENT FAILED!")
            print("=" * 50)
            print("üîç Check logs in automation.log")

    except (ValueError, KeyboardInterrupt) as e:
        print(f"\n‚ùå Error: {e}")
        print("üö™ Exiting...")
        return
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        return


def main():
    """G≈Ç√≥wna funkcja"""
    parser = argparse.ArgumentParser(description='LLM Testing Automation')
    parser.add_argument('--model', help='Model name (gpt-4.5, claude-3.7-sonnet, etc.)')
    parser.add_argument('--strategy', choices=['simple_prompting', 'chain_of_thought_prompting'],
                        help='Prompting strategy')
    parser.add_argument('--context', choices=['interface', 'interface_docstring', 'full_context'],
                        help='Code context level')
    parser.add_argument('--config', help='JSON config file for batch experiments')
    parser.add_argument('--headless', action='store_true', help='Run browser in headless mode')
    parser.add_argument('--no-headless', action='store_true', help='Run browser with GUI (default)')

    args = parser.parse_args()

    setup_logging()
    logger = logging.getLogger(__name__)

    runner = ExperimentRunner()

    # Okre≈õl tryb przeglƒÖdarki
    headless = args.headless if args.headless else not args.no_headless
    if not args.headless and not args.no_headless:
        headless = False  # Domy≈õlnie GUI dla ≈Çatwiejszego debugowania

    if args.config:
        # Batch mode
        logger.info(f"Running batch experiments from {args.config}")
        results = runner.run_batch_experiments(args.config)

        # Zapisz wyniki batch
        batch_results_file = Path("batch_results.json")
        with open(batch_results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        logger.info(f"Batch results saved to {batch_results_file}")

    elif args.model and args.strategy and args.context:
        # Single experiment mode
        logger.info(f"Running single experiment: {args.model} - {args.strategy} - {args.context}")

        result = runner.run_single_experiment(
            args.model, args.strategy, args.context, headless=headless
        )

        if result:
            logger.info("Experiment completed successfully")
            print("\n=== EXPERIMENT RESULTS ===")
            print(f"Model: {args.model}")
            print(f"Strategy: {args.strategy}")
            print(f"Context: {args.context}")

            if 'analysis' in result and result['analysis']:
                analysis = result['analysis']
                if 'compilation_success' in analysis:
                    print(f"Compilation: {'‚úì' if analysis['compilation_success'] else '‚úó'}")

                if 'coverage' in analysis and analysis['coverage']:
                    cov = analysis['coverage']
                    print(f"Coverage: {cov.get('coverage_percent', 'N/A')}%")

                if 'scenarios' in analysis and analysis['scenarios']:
                    scen = analysis['scenarios']
                    print(f"Test methods: {scen.get('total_test_methods', 'N/A')}")
        else:
            logger.error("Experiment failed")

    else:
        # Interactive mode
        interactive_mode(runner)


if __name__ == "__main__":
    main()