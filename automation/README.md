# LLM Test Generation Automation

Automated system for evaluating LLM-generated unit tests using different prompting strategies and code contexts.

## ğŸ“ Directory Structure

```
automation/
â”œâ”€â”€ cli_automation/              # CLI-based automation (subprocess)
â”‚   â”œâ”€â”€ base_cli_client.py      # Base CLI client
â”‚   â”œâ”€â”€ claude_code_client.py   # Claude Code CLI client
â”‚   â””â”€â”€ gemini_cli_client.py    # Gemini CLI client
â”œâ”€â”€ cli_results/                 # CLI automation results
â”œâ”€â”€ prompts_results/             # Legacy results directory
â”œâ”€â”€ experiment_runner.py         # Analysis pipeline (shared)
â”œâ”€â”€ prompt_strategies.py         # Prompting strategies
â”œâ”€â”€ cli_experiment_runner.py    # CLI automation entry point
â”œâ”€â”€ run_mutmut_backfill.py      # Mutation testing backfill (Windowsâ†’WSL)
â”œâ”€â”€ aggregate_runs.py           # Aggregate multiple runs
â”œâ”€â”€ cli_config.json             # CLI automation config
â”œâ”€â”€ gemini_cli_config.json      # Gemini CLI config
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Quick Start

### CLI Automation

**List available models:**
```bash
cd automation
python cli_experiment_runner.py --list-models
```

**Run single experiment:**
```bash
python cli_experiment_runner.py \
  --model claude-code-sonnet-4.5 \
  --strategy simple_prompting \
  --context interface
```

**Run batch experiments:**
```bash
python cli_experiment_runner.py --config cli_config.json
```

## ğŸ§ª Mutation Testing

**On Windows:** Mutation testing is automatically skipped (requires fork support).

**Backfill from WSL/Linux:**
```bash
# From WSL
cd /mnt/c/Users/.../LLM-analysis/automation
python3 run_mutmut_backfill.py --results-dir cli_results
```

## ğŸ“Š Results Structure

Results are organized in `cli_results/{strategy}/{context}/{model}/run_XXX/`

Each result directory contains:
- `tests.py` - Generated test code
- `mutmut_test.py` - Filtered tests for mutation testing
- `order_calculator.py` - Source code under test
- `experiment_results.json` - Experiment metadata
- `analysis_results.json` - Coverage, mutation, quality metrics
- `podsumowanie-{model}.md` - Human-readable summary
- `htmlcov/` - Coverage report

## ğŸ”§ Prompting Strategies

**Simple Prompting:** Single prompt requesting tests
**Chain of Thought:** Three-step process (analyze â†’ plan â†’ implement)

## ğŸ“ Context Levels

- **interface:** Method signatures only
- **interface_docstring:** Signatures + docstrings
- **full_context:** Complete implementation

## ğŸ› ï¸ Development

**Install dependencies:**
```bash
pip install -r requirements.txt
```

**CLI automation requirements:**
- Claude Code: Already installed (you're using it!)
- Google Gemini: `npm install -g @google/gemini-cli`

## ğŸ¯ Research Metrics

The system automatically measures:
- **Compilation success rate**
- **Statement coverage** (line coverage)
- **Branch coverage** (decision coverage)
- **Mutation score** (% mutants killed)
- **Test count** (methods generated)
- **Test success rate** (passing vs total)
- **Test quality metrics** (assertions, error handling, duplicates)
- **Code smells** (independence, naming, complexity)

All metrics saved in JSON, CSV, and Markdown formats.

## âš ï¸ Important Notes

### File Locations
- Run all scripts from `automation/` directory
- `order_calculator.py` is in repository root
- Results stay in `cli_results/`

### Mutation Testing
- **Requires WSL/Linux** (fork support needed)
- Use `run_mutmut_backfill.py` to add mutation results later
- Automatically filters to passing tests only

## ğŸ“„ License

Research project - see repository root for license.
